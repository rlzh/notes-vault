#general  #informationtheory 
# Entropy
- A concept most commonly associated with state of disorder, randomness, or uncertainty
- In Information Theory, the entropy of a [[Statistics#Random Variable|random variable]] quantifies the average level of uncertainty or information associated with the variable's potential states or possible outcomes 
- **==Measures the [[Statistics#Expected value|expected]] amount of information needed to describe the state of the variable**== (considering the distribution of probabilities across all potential states)
